{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a974cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, GlobalAveragePooling1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "from transformers import TFBertModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "DATA_PATH = './data'\n",
    "SOLUTION_PATH = './solutions'\n",
    "WEIGHTS_PATH = './model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6993d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3d50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0434a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301bd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    \n",
    "    text = re.sub(r\"thats\", \" that is \", text)\n",
    "    text = re.sub(r\"youre\", \" you are \", text)\n",
    "    text = re.sub(r\"cant\", \" can not \", text)\n",
    "    text = re.sub(r\"didnt\", \" did not \", text)\n",
    "    text = re.sub(r\"dont\", \" do not \", text)\n",
    "    text = re.sub(r\"doesnt\", \" does not \", text)\n",
    "    text = re.sub(r\"ive\", \" i have \", text)\n",
    "    text = re.sub(r\"youve\", \" you have \", text)\n",
    "    text = re.sub(r\"wont\", \" will not \", text)\n",
    "    text = re.sub(r\"hes\", \" he is \", text)\n",
    "    text = re.sub(r\"isnt\", \" is not \", text)\n",
    "    text = re.sub(r\"havent\", \" have not \", text)\n",
    "    text = re.sub(r\"arent\", \" are not \", text)\n",
    "    text = re.sub(r\"whats\", \" what is \", text)\n",
    "    text = re.sub(r\"wasnt\", \" was not \", text)\n",
    "    text = re.sub(r\"theres\", \" there is \", text)\n",
    "    text = re.sub(r\"youll\", \" you will \", text)\n",
    "    text = re.sub(r\"wouldnt\", \" would not \", text)\n",
    "    text = re.sub(r\"shouldnt\", \" should not \", text)\n",
    "    text = re.sub(r\"theyre\", \" they are \", text)\n",
    "\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbb87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].map(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6e472d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].str.lower().str.replace(r'[^\\w\\s]+', '', regex=True)\n",
    "train['hash'] = train['comment_text'].str[:200].apply(hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8b9695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307958, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop_duplicates(subset=['hash'])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa95a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed268d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            15065.0\n",
       "severe_toxic      1560.0\n",
       "obscene           8313.0\n",
       "threat             465.0\n",
       "insult            7747.0\n",
       "identity_hate     1380.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.columns[2:]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4f0ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 01:27:55.572238: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "max_length = 120\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name, output_hidden_states=True, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47607bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = auto_tokenizer.get_vocab()\n",
    "counter = Counter()\n",
    "temp = train.loc[train[train.columns[2:]].sum(axis=1) > 0, 'comment_text']\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    sentence = temp.iloc[i]\n",
    "    words = sentence.split()[:]\n",
    "    \n",
    "    for word in words:\n",
    "        if word.strip() in vocab:\n",
    "            pass\n",
    "        else:\n",
    "            counter[word.strip()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e49e5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faggot', 1985),\n",
       " ('nigger', 1964),\n",
       " ('cunt', 1538),\n",
       " ('wiki', 1467),\n",
       " ('moron', 1143),\n",
       " ('fag', 1015),\n",
       " ('vandalism', 985),\n",
       " ('wanker', 807),\n",
       " ('dickhead', 770),\n",
       " ('admin', 753),\n",
       " ('edits', 724),\n",
       " ('faggots', 677),\n",
       " ('fucksex', 624),\n",
       " ('yourselfgo', 621),\n",
       " ('twat', 611),\n",
       " ('delete', 546),\n",
       " ('poop', 503),\n",
       " ('buttsecks', 498),\n",
       " ('bitc', 483),\n",
       " ('noobs', 460)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d641951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dont have embedding for many common slur words\n",
    "for word in ['retard', 'nigger', 'cunt', 'twat', 'moron', 'wanker', 'faggot', 'cocksucker']:\n",
    "    if word in vocab:\n",
    "        print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
